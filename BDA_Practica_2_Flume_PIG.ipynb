{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Aplicado - Práctica 2 - Hadoop+Flume+PIG\n",
        "\n",
        "Se cargan datos con Flume a partir de la técnica de spooldir, para su salida transformarla y cargarla en fichero de HDFS con PIG."
      ],
      "metadata": {
        "id": "BtaqVRgWHpl5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHjARS6Fe_cR"
      },
      "source": [
        "## Instalación de herramientas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbHs3DMyjIm"
      },
      "source": [
        "### Instalación de Hadoop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT47p0BgeQII"
      },
      "source": [
        "Descargamos Hadoop, descomprimimos y movemos a la carpeta de programas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MmQb-EBbyS4o"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz\n",
        "tar -xzf hadoop-3.3.1.tar.gz\n",
        "mv hadoop-3.3.1/ /usr/local/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQSHgHl-yg0o"
      },
      "source": [
        "Creamos las variables de entorno necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MzexlzsKyc4n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/usr/local/hadoop-3.3.1/bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpxu9EUneYES"
      },
      "source": [
        "### Instalación de Flume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LImQnBZK6N4x"
      },
      "source": [
        "Descargamos Flume, descomprimimos y movemos a la carpeta de programas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WAF8S7iyKF2r"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -q https://dlcdn.apache.org/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz\n",
        "tar xzf apache-flume-1.9.0-bin.tar.gz\n",
        "mv apache-flume-1.9.0-bin/ /usr/local/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJeqX4GR6P0I"
      },
      "source": [
        "Creamos las variables de entorno necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ofaLnVMhKvM4"
      },
      "outputs": [],
      "source": [
        "os.environ[\"FLUME_HOME\"] = \"/usr/local/apache-flume-1.9.0-bin\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/usr/local/apache-flume-1.9.0-bin/bin\"\n",
        "os.environ[\"JAVA_OPTS\"]= \"-Xms400m -Xmx3000m -Dcom.sun.management.jmxremote\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vljTmO16yPNv"
      },
      "source": [
        "### Instalación de PIG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UffUOvh8Yf1W"
      },
      "source": [
        "Descargamos PIG, descomprimimos y movemos a la carpeta de programas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KABG4warYf1X"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -q https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz\n",
        "tar -xzf pig-0.17.0.tar.gz\n",
        "mv pig-0.17.0/ /usr/local/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMlRDuPyYf1Y"
      },
      "source": [
        "Creamos las variables de entorno necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l5oxNZ--Yf1Y"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PIG_HOME\"] = \"/usr/local/pig-0.17.0\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/usr/local/pig-0.17.0/bin\"\n",
        "os.environ[\"PIG_CLASSPATH\"] = \"/usr/local/hadoop-3.3.1/conf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBwJqt3y6Sme"
      },
      "source": [
        "### Comprobación de instalación\n",
        "\n",
        "Ejecutamos los comandos para ver las distintas versiones instaladas de las herramientas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhRCjvZCoS9Y",
        "outputId": "2a30e6ab-f93b-4b59-e0dc-ca9148cfdb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.1\n",
            "Source code repository https://github.com/apache/hadoop.git -r a3b9c37a397ad4188041dd80621bdeefc46885f2\n",
            "Compiled by ubuntu on 2021-06-15T05:13Z\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 88a4ddb2299aca054416d6b7f81ca55\n",
            "This command was run using /usr/local/hadoop-3.3.1/share/hadoop/common/hadoop-common-3.3.1.jar\n"
          ]
        }
      ],
      "source": [
        "!hadoop version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiENUy7tLD0r",
        "outputId": "e3117e4b-daa3-445b-bda9-b2e4294fd7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flume 1.9.0\n",
            "Source code repository: https://git-wip-us.apache.org/repos/asf/flume.git\n",
            "Revision: d4fcab4f501d41597bc616921329a4339f73585e\n",
            "Compiled by fszabo on Mon Dec 17 20:45:25 CET 2018\n",
            "From source with checksum 35db629a3bda49d23e9b3690c80737f9\n"
          ]
        }
      ],
      "source": [
        "!flume-ng version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR58E5KPYf1Z",
        "outputId": "5b1f26f0-78e5-4b94-e1e6-b50d9f6e7f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Pig version 0.17.0 (r1797386) \n",
            "compiled Jun 02 2017, 15:41:58\n"
          ]
        }
      ],
      "source": [
        "!pig -version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMS4lfgfmeq"
      },
      "source": [
        "## Ejecución de Flume por Spooling Dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación de datos"
      ],
      "metadata": {
        "id": "kBUNZCdsZN7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos un CSV que contiene 'Indicadores clave de enfermedades cardíacas 2020' (utilizado en la práctica de Sistemas de Big Data), renombrándolo a *datos.csv*.\n",
        "\n",
        "Enlace a la fuente original: https://www.kaggle.com/kamilpytlak/personal-key-indicators-of-heart-disease"
      ],
      "metadata": {
        "id": "k3rUebCEPmNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SLzm6vUwRpI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71d7a72-6cbf-435a-8214-b8f0868dec54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_nupX9-MOto1mTcM7RsHzM4PzOZXnnT6\n",
            "To: /content/datos.csv\n",
            "100% 25.2M/25.2M [00:00<00:00, 63.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown -O datos.csv \"1_nupX9-MOto1mTcM7RsHzM4PzOZXnnT6\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a que la descarga de datos, a veces no es capaz de realizarla, por seguridad se incluyen los siguientes borrados de directorios que tienen que estar vacíos o no tienen que existir para un correcto funcionamiento de una posible re-ejecución."
      ],
      "metadata": {
        "id": "r7wsCf1_B2O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf entrada_datos/\n",
        "rm -rf salida/\n",
        "rm -rf results/"
      ],
      "metadata": {
        "id": "EJnOyVS8B2A2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EuTteEWfqdy"
      },
      "source": [
        "Creamos el directorio de entrada que monitorizará el agente de Flume para el *'spooling dir'*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8Ynzv-nPQO7A"
      },
      "outputs": [],
      "source": [
        "!mkdir -p entrada_datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como el fichero tiene más de 300 mil registros, vamos quedarnos solo con los 10 mil primeros, eliminando la primera línea de cabecera y generándose en la carpeta creada anteriormente."
      ],
      "metadata": {
        "id": "pRszmo7oQuu-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0Rw0f3t_ysTk"
      },
      "outputs": [],
      "source": [
        "!head -10001 datos.csv > /content/entrada_datos/datos.csv \n",
        "!sed -i 1d /content/entrada_datos/datos.csv "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos que el fichero se ha creado, listando el contenido del directorio."
      ],
      "metadata": {
        "id": "xmMI7oF3RUGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50IpJCeKcJMU",
        "outputId": "b5c81808-715b-4d9f-89c6-efa36ae8271e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root     797964 2022-05-05 11:47 entrada_datos/datos.csv\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls entrada_datos/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agente Flume"
      ],
      "metadata": {
        "id": "KTe4gB1rZUOh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHECp_Ulfmbo"
      },
      "source": [
        "Creamos el fichero de configuración del agente de Flume, donde indicamos como entrada que es de tipo *spooldir* y el directorio que tiene que monitorizar. Como canal indicamos que será de tipo fichero. Y como sumidero ponemos que el tipo será HDFS, el directorio donde se guardará (lo crea Flume directamente si no existe), la escritura del fichero será en modo texto, y añadimos varias propiedades *roll* para que genere ficheros más grandes de lo definido por defecto.<br> \n",
        "Si no se definen estas últimas propiedades, se generará un fichero por cada 10-20 líneas de entrada, por lo que, a efectos de visualización, se complica mostrarlo todo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT__C334QElx",
        "outputId": "006f1ddf-a57d-4d24-9a23-e83324075ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting monitor_flume.conf\n"
          ]
        }
      ],
      "source": [
        "%%writefile monitor_flume.conf\n",
        "# Configuración del Agente para monitorización de un directorio\n",
        "\n",
        "#Definición del agente: nombre y componentes\n",
        "agente.sources  = s1  \n",
        "agente.sinks    = k1  \n",
        "agente.channels = c1  \n",
        "   \n",
        "#Configuración de propiedades Source\n",
        "agente.sources.s1.type              = spooldir\n",
        "agente.sources.s1.spoolDir          = /content/entrada_datos\n",
        "\n",
        "# Configuración de propiedades del canal\n",
        "agente.channels.c1.type = file\n",
        "\n",
        "# Configuración de propiedades del sink\n",
        "agente.sinks.k1.type = hdfs\n",
        "agente.sinks.k1.hdfs.path = /content/salida\n",
        "agente.sinks.k1.hdfs.fileType = DataStream\n",
        "agente.sinks.k1.hdfs.writeFormat = Text\n",
        "agente.sinks.k1.hdfs.rollInterval = 0\n",
        "agente.sinks.k1.hdfs.rollSize = 81920\n",
        "agente.sinks.k1.hdfs.rollCount = 0\n",
        "\n",
        "#Vinculación de source y sink al canal creado\n",
        "agente.sources.s1.channels = c1\n",
        "agente.sinks.k1.channel    = c1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV2wZelEgNDK"
      },
      "source": [
        "Ejecución del agente Flume.\n",
        "\n",
        "MATERIAL EXTRA PROPIO<br>\n",
        "Como Flume se queda ejecutando \"ad eternum\" y hay que manualmente 'Interrumpir Ejecución' ya sea parándolo en la misma celda o por el menú, se ha creado un script para no tener que preocuparse por este motivo.<br>\n",
        "Primero ejecutamos nuestro agente, pero enviando su ejecución a background, liberando así la sesión, y guardando su salida por pantalla en un fichero (*agente.out*).<br>\n",
        "Hacemos que espere durante 60 segundos con el comando *sleep*, para después proceder a matar la ejecución del agente. Como este se ejecuta como una instancia de *java*, pues es lo que debemos matar.<br>\n",
        "Por último, mostramos la salida del agente que habíamos guardado en fichero para comprobar que se han realizado todos los pasos correctamente.<br>\n",
        "Durante las pruebas, entre 10 y 30 segundos se realiza el procesamiento de los datos, pero se ha puesto a 60 segundos solo por asegurar, por si la máquina estuviese algo más lenta."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nohup flume-ng agent --conf ./ -f ./monitor_flume.conf -n agente -Dflume.root.logger=INFO -Xmx3000m > agente.out &\n",
        "sleep 60\n",
        "pkill java\n",
        "cat agente.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhfc6i_o5HLV",
        "outputId": "653b7a0b-0563-45f5-a039-0e86f962c17b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Info: Including Hadoop libraries found via (/usr/local/hadoop-3.3.1/bin/hadoop) for HDFS access\n",
            "Info: Including Hive libraries found via () for Hive access\n",
            "+ exec /usr/lib/jvm/java-11-openjdk-amd64//bin/java -Xmx20m -Dflume.root.logger=INFO -Xmx3000m -cp '/content:/usr/local/apache-flume-1.9.0-bin/lib/*:/usr/local/hadoop-3.3.1/etc/hadoop:/usr/local/hadoop-3.3.1/share/hadoop/common/lib/*:/usr/local/hadoop-3.3.1/share/hadoop/common/*:/usr/local/hadoop-3.3.1/share/hadoop/hdfs:/usr/local/hadoop-3.3.1/share/hadoop/hdfs/lib/*:/usr/local/hadoop-3.3.1/share/hadoop/hdfs/*:/usr/local/hadoop-3.3.1/share/hadoop/mapreduce/*:/usr/local/hadoop-3.3.1/share/hadoop/yarn:/usr/local/hadoop-3.3.1/share/hadoop/yarn/lib/*:/usr/local/hadoop-3.3.1/share/hadoop/yarn/*:/lib/*' -Djava.library.path=:/usr/local/hadoop-3.3.1/lib/native org.apache.flume.node.Application -f ./monitor_flume.conf -n agente\n",
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/local/apache-flume-1.9.0-bin/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
            "2022-05-05 11:47:32,030 INFO node.PollingPropertiesFileConfigurationProvider: Configuration provider starting\n",
            "2022-05-05 11:47:32,044 INFO node.PollingPropertiesFileConfigurationProvider: Reloading configuration file:./monitor_flume.conf\n",
            "2022-05-05 11:47:32,052 INFO conf.FlumeConfiguration: Processing:c1\n",
            "2022-05-05 11:47:32,054 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,054 INFO conf.FlumeConfiguration: Added sinks: k1 Agent: agente\n",
            "2022-05-05 11:47:32,055 INFO conf.FlumeConfiguration: Processing:s1\n",
            "2022-05-05 11:47:32,056 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,056 INFO conf.FlumeConfiguration: Processing:s1\n",
            "2022-05-05 11:47:32,056 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,056 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,056 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,057 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,057 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,057 INFO conf.FlumeConfiguration: Processing:k1\n",
            "2022-05-05 11:47:32,057 INFO conf.FlumeConfiguration: Processing:s1\n",
            "2022-05-05 11:47:32,057 WARN conf.FlumeConfiguration: Agent configuration for 'agente' has no configfilters.\n",
            "2022-05-05 11:47:32,098 INFO conf.FlumeConfiguration: Post-validation flume configuration contains configuration for agents: [agente]\n",
            "2022-05-05 11:47:32,098 INFO node.AbstractConfigurationProvider: Creating channels\n",
            "2022-05-05 11:47:32,111 INFO channel.DefaultChannelFactory: Creating instance of channel c1 type file\n",
            "2022-05-05 11:47:32,138 INFO node.AbstractConfigurationProvider: Created channel c1\n",
            "2022-05-05 11:47:32,139 INFO source.DefaultSourceFactory: Creating instance of source s1, type spooldir\n",
            "2022-05-05 11:47:32,160 INFO sink.DefaultSinkFactory: Creating instance of sink: k1, type: hdfs\n",
            "2022-05-05 11:47:32,169 INFO node.AbstractConfigurationProvider: Channel c1 connected to [s1, k1]\n",
            "2022-05-05 11:47:32,177 INFO node.Application: Starting new configuration:{ sourceRunners:{s1=EventDrivenSourceRunner: { source:Spool Directory source s1: { spoolDir: /content/entrada_datos } }} sinkRunners:{k1=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2b748ffc counterGroup:{ name:null counters:{} } }} channels:{c1=FileChannel c1 { dataDirs: [/root/.flume/file-channel/data] }} }\n",
            "2022-05-05 11:47:32,177 INFO node.Application: Starting Channel c1\n",
            "2022-05-05 11:47:32,178 INFO file.FileChannel: Starting FileChannel c1 { dataDirs: [/root/.flume/file-channel/data] }...\n",
            "2022-05-05 11:47:32,386 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.\n",
            "2022-05-05 11:47:32,388 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started\n",
            "2022-05-05 11:47:32,399 INFO file.Log: Encryption is not enabled\n",
            "2022-05-05 11:47:32,401 INFO file.Log: Replay started\n",
            "2022-05-05 11:47:32,416 INFO file.Log: Found NextFileID 1, from [/root/.flume/file-channel/data/log-1]\n",
            "2022-05-05 11:47:32,425 INFO file.EventQueueBackingStoreFileV3: Starting up with /root/.flume/file-channel/checkpoint/checkpoint and /root/.flume/file-channel/checkpoint/checkpoint.meta\n",
            "2022-05-05 11:47:32,425 INFO file.EventQueueBackingStoreFileV3: Reading checkpoint metadata from /root/.flume/file-channel/checkpoint/checkpoint.meta\n",
            "2022-05-05 11:47:32,548 INFO file.FlumeEventQueue: QueueSet population inserting 0 took 0\n",
            "2022-05-05 11:47:32,559 INFO file.Log: Last Checkpoint Thu May 05 11:39:54 UTC 2022, queue depth = 0\n",
            "2022-05-05 11:47:32,565 INFO file.Log: Replaying logs with v2 replay logic\n",
            "2022-05-05 11:47:32,569 INFO file.ReplayHandler: Starting replay of [/root/.flume/file-channel/data/log-1]\n",
            "2022-05-05 11:47:32,571 INFO file.ReplayHandler: Replaying /root/.flume/file-channel/data/log-1\n",
            "2022-05-05 11:47:32,578 INFO tools.DirectMemoryUtils: Unable to get maxDirectMemory from VM: ClassNotFoundException: sun.misc.VM\n",
            "2022-05-05 11:47:32,585 INFO tools.DirectMemoryUtils: Direct Memory Allocation:  Allocation = 1048576, Allocated = 0, MaxDirectMemorySize = 3145728000, Remaining = 3145728000\n",
            "2022-05-05 11:47:32,630 INFO file.LogFile: fast-forward to checkpoint position: 157\n",
            "2022-05-05 11:47:32,631 INFO file.LogFile: Encountered EOF at 157 in /root/.flume/file-channel/data/log-1\n",
            "2022-05-05 11:47:32,631 INFO file.ReplayHandler: read: 0, put: 0, take: 0, rollback: 0, commit: 0, skip: 0, eventCount:0\n",
            "2022-05-05 11:47:32,631 INFO file.FlumeEventQueue: Search Count = 0, Search Time = 0, Copy Count = 0, Copy Time = 0\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.mapdb.Volume$ByteBufferVol (file:/usr/local/apache-flume-1.9.0-bin/lib/mapdb-0.9.9.jar) to method java.nio.DirectByteBuffer.cleaner()\n",
            "WARNING: Please consider reporting this to the maintainers of org.mapdb.Volume$ByteBufferVol\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "2022-05-05 11:47:32,649 INFO file.Log: Rolling /root/.flume/file-channel/data\n",
            "2022-05-05 11:47:32,649 INFO file.Log: Roll start /root/.flume/file-channel/data\n",
            "2022-05-05 11:47:32,651 INFO file.LogFile: Opened /root/.flume/file-channel/data/log-2\n",
            "2022-05-05 11:47:32,666 INFO file.Log: Roll end\n",
            "2022-05-05 11:47:32,667 INFO file.EventQueueBackingStoreFile: Start checkpoint for /root/.flume/file-channel/checkpoint/checkpoint, elements to sync = 0\n",
            "2022-05-05 11:47:32,682 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1651751252455, queueSize: 0, queueHead: 0\n",
            "2022-05-05 11:47:32,712 INFO file.Log: Updated checkpoint for file: /root/.flume/file-channel/data/log-2 position: 0 logWriteOrderID: 1651751252455\n",
            "2022-05-05 11:47:32,712 INFO file.FileChannel: Queue Size after replay: 0 [channel=c1]\n",
            "2022-05-05 11:47:32,712 INFO node.Application: Starting Sink k1\n",
            "2022-05-05 11:47:32,717 INFO node.Application: Starting Source s1\n",
            "2022-05-05 11:47:32,723 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.\n",
            "2022-05-05 11:47:32,723 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started\n",
            "2022-05-05 11:47:32,726 INFO source.SpoolDirectorySource: SpoolDirectorySource source starting with directory: /content/entrada_datos\n",
            "2022-05-05 11:47:32,740 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: s1: Successfully registered new MBean.\n",
            "2022-05-05 11:47:32,740 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: s1 started\n",
            "2022-05-05 11:47:33,760 INFO hdfs.HDFSDataStream: Serializer = TEXT, UseRawLocalFileSystem = false\n",
            "2022-05-05 11:47:34,165 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253759.tmp\n",
            "2022-05-05 11:47:34,216 INFO avro.ReliableSpoolingFileEventReader: Last read took us just up to a file boundary. Rolling to the next file, if there is one.\n",
            "2022-05-05 11:47:34,217 INFO avro.ReliableSpoolingFileEventReader: Preparing to move file /content/entrada_datos/datos.csv to /content/entrada_datos/datos.csv.COMPLETED\n",
            "2022-05-05 11:47:35,050 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:35,064 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:35,551 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253759.tmp\n",
            "2022-05-05 11:47:35,557 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253759.tmp to /content/salida/FlumeData.1651751253759\n",
            "2022-05-05 11:47:35,598 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253760.tmp\n",
            "2022-05-05 11:47:35,647 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:35,648 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:35,867 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253760.tmp\n",
            "2022-05-05 11:47:35,868 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253760.tmp to /content/salida/FlumeData.1651751253760\n",
            "2022-05-05 11:47:35,906 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253761.tmp\n",
            "2022-05-05 11:47:35,946 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:35,946 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:36,186 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253761.tmp\n",
            "2022-05-05 11:47:36,189 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253761.tmp to /content/salida/FlumeData.1651751253761\n",
            "2022-05-05 11:47:36,220 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253762.tmp\n",
            "2022-05-05 11:47:36,261 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:36,261 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:36,392 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253762.tmp\n",
            "2022-05-05 11:47:36,394 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253762.tmp to /content/salida/FlumeData.1651751253762\n",
            "2022-05-05 11:47:36,430 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253763.tmp\n",
            "2022-05-05 11:47:36,462 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:36,463 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:36,581 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253763.tmp\n",
            "2022-05-05 11:47:36,582 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253763.tmp to /content/salida/FlumeData.1651751253763\n",
            "2022-05-05 11:47:36,617 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253764.tmp\n",
            "2022-05-05 11:47:36,656 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:36,656 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:36,759 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253764.tmp\n",
            "2022-05-05 11:47:36,760 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253764.tmp to /content/salida/FlumeData.1651751253764\n",
            "2022-05-05 11:47:36,790 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253765.tmp\n",
            "2022-05-05 11:47:36,827 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:36,827 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:36,936 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253765.tmp\n",
            "2022-05-05 11:47:36,937 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253765.tmp to /content/salida/FlumeData.1651751253765\n",
            "2022-05-05 11:47:36,972 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253766.tmp\n",
            "2022-05-05 11:47:37,004 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:37,005 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:37,129 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253766.tmp\n",
            "2022-05-05 11:47:37,130 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253766.tmp to /content/salida/FlumeData.1651751253766\n",
            "2022-05-05 11:47:37,195 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253767.tmp\n",
            "2022-05-05 11:47:37,231 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:37,231 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:47:37,343 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253767.tmp\n",
            "2022-05-05 11:47:37,344 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253767.tmp to /content/salida/FlumeData.1651751253767\n",
            "2022-05-05 11:47:37,370 INFO hdfs.BucketWriter: Creating /content/salida/FlumeData.1651751253768.tmp\n",
            "2022-05-05 11:47:37,397 INFO hdfs.AbstractHDFSWriter: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer; err=java.lang.NoSuchMethodException: org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.getNumCurrentReplicas()\n",
            "2022-05-05 11:47:37,397 INFO hdfs.BucketWriter: isFileClosed() is not available in the version of the distributed filesystem being used. Flume will not attempt to re-close files if the close fails on the first attempt\n",
            "2022-05-05 11:48:02,401 INFO file.EventQueueBackingStoreFile: Start checkpoint for /root/.flume/file-channel/checkpoint/checkpoint, elements to sync = 10000\n",
            "2022-05-05 11:48:02,417 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1651751272656, queueSize: 0, queueHead: 9998\n",
            "2022-05-05 11:48:02,438 INFO file.Log: Updated checkpoint for file: /root/.flume/file-channel/data/log-2 position: 1645212 logWriteOrderID: 1651751272656\n",
            "2022-05-05 11:48:30,374 INFO node.Application: Shutting down configuration: { sourceRunners:{s1=EventDrivenSourceRunner: { source:Spool Directory source s1: { spoolDir: /content/entrada_datos } }} sinkRunners:{k1=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2b748ffc counterGroup:{ name:null counters:{runner.backoffs.consecutive=13, runner.backoffs=14} } }} channels:{c1=FileChannel c1 { dataDirs: [/root/.flume/file-channel/data] }} }\n",
            "2022-05-05 11:48:30,379 INFO node.Application: Stopping Source s1\n",
            "2022-05-05 11:48:30,379 INFO lifecycle.LifecycleSupervisor: Stopping component: EventDrivenSourceRunner: { source:Spool Directory source s1: { spoolDir: /content/entrada_datos } }\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: s1 stopped\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. source.start.time == 1651751252740\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. source.stop.time == 1651751310382\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.append-batch.accepted == 100\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.append-batch.received == 100\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.append.accepted == 0\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.append.received == 0\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.channel.write.fail == 0\n",
            "2022-05-05 11:48:30,382 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.event.read.fail == 0\n",
            "2022-05-05 11:48:30,383 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.events.accepted == 10000\n",
            "2022-05-05 11:48:30,383 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.events.received == 10000\n",
            "2022-05-05 11:48:30,383 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.generic.processing.fail == 0\n",
            "2022-05-05 11:48:30,383 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SOURCE, name: s1. src.open-connection.count == 0\n",
            "2022-05-05 11:48:30,383 INFO source.SpoolDirectorySource: SpoolDir source s1 stopped. Metrics: SOURCE:s1{src.events.accepted=10000, src.open-connection.count=0, src.append.received=0, src.channel.write.fail=0, src.append-batch.received=100, src.generic.processing.fail=0, src.append-batch.accepted=100, src.append.accepted=0, src.events.received=10000, src.event.read.fail=0}\n",
            "2022-05-05 11:48:30,383 INFO node.Application: Stopping Sink k1\n",
            "2022-05-05 11:48:30,383 INFO lifecycle.LifecycleSupervisor: Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2b748ffc counterGroup:{ name:null counters:{runner.backoffs.consecutive=13, runner.backoffs=14} } }\n",
            "2022-05-05 11:48:30,388 INFO hdfs.HDFSEventSink: Closing /content/salida/FlumeData\n",
            "2022-05-05 11:48:30,388 INFO hdfs.BucketWriter: Closing /content/salida/FlumeData.1651751253768.tmp\n",
            "2022-05-05 11:48:30,390 INFO hdfs.BucketWriter: Renaming /content/salida/FlumeData.1651751253768.tmp to /content/salida/FlumeData.1651751253768\n",
            "2022-05-05 11:48:30,397 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 stopped\n",
            "2022-05-05 11:48:30,397 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.start.time == 1651751252723\n",
            "2022-05-05 11:48:30,398 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.stop.time == 1651751310397\n",
            "2022-05-05 11:48:30,398 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.batch.complete == 100\n",
            "2022-05-05 11:48:30,398 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.batch.empty == 14\n",
            "2022-05-05 11:48:30,398 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.batch.underflow == 0\n",
            "2022-05-05 11:48:30,398 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.channel.read.fail == 0\n",
            "2022-05-05 11:48:30,398 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.connection.closed.count == 10\n",
            "2022-05-05 11:48:30,401 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.connection.creation.count == 10\n",
            "2022-05-05 11:48:30,402 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.connection.failed.count == 0\n",
            "2022-05-05 11:48:30,402 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.event.drain.attempt == 10000\n",
            "2022-05-05 11:48:30,402 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.event.drain.sucess == 10000\n",
            "2022-05-05 11:48:30,402 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: SINK, name: k1. sink.event.write.fail == 0\n",
            "2022-05-05 11:48:30,402 INFO node.Application: Stopping Channel c1\n",
            "2022-05-05 11:48:30,403 INFO lifecycle.LifecycleSupervisor: Stopping component: FileChannel c1 { dataDirs: [/root/.flume/file-channel/data] }\n",
            "2022-05-05 11:48:30,403 INFO file.FileChannel: Stopping FileChannel c1 { dataDirs: [/root/.flume/file-channel/data] }...\n",
            "2022-05-05 11:48:30,406 INFO file.EventQueueBackingStoreFile: Start checkpoint for /root/.flume/file-channel/checkpoint/checkpoint, elements to sync = 0\n",
            "2022-05-05 11:48:30,423 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1651751272657, queueSize: 0, queueHead: 9998\n",
            "2022-05-05 11:48:30,438 INFO file.Log: Updated checkpoint for file: /root/.flume/file-channel/data/log-2 position: 1645212 logWriteOrderID: 1651751272657\n",
            "2022-05-05 11:48:30,439 INFO file.Log: Attempting to shutdown background worker.\n",
            "2022-05-05 11:48:30,439 INFO file.LogFile: Closing /root/.flume/file-channel/data/log-2\n",
            "2022-05-05 11:48:30,440 INFO file.LogFile: Closing RandomReader /root/.flume/file-channel/data/log-1\n",
            "2022-05-05 11:48:30,446 INFO file.LogFile: Closing RandomReader /root/.flume/file-channel/data/log-2\n",
            "2022-05-05 11:48:30,452 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 stopped\n",
            "2022-05-05 11:48:30,452 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.start.time == 1651751252388\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.stop.time == 1651751310452\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.capacity == 1000000\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.current.size == 0\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.event.put.attempt == 10000\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.event.put.success == 10000\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.event.take.attempt == 10014\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.event.take.success == 10000\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.file.checkpoint.backup.write.error == 0\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.file.checkpoint.write.error == 0\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.file.event.put.error == 0\n",
            "2022-05-05 11:48:30,453 INFO instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: c1. channel.file.event.take.error == 0\n",
            "2022-05-05 11:48:30,453 INFO lifecycle.LifecycleSupervisor: Stopping lifecycle supervisor 12\n",
            "2022-05-05 11:48:30,456 INFO node.PollingPropertiesFileConfigurationProvider: Configuration provider stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN3_HeqAG4DL"
      },
      "source": [
        "Comprobamos que se han creado los ficheros en el directorio `salida`, listando en directorio y viendo el contenido de las primeras líneas de los ficheros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XztFNYXYUCTG",
        "outputId": "ba08fce2-e245-4557-af4d-6a6ba9db222f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 items\n",
            "-rw-r--r--   1 root root      83044 2022-05-05 11:47 /content/salida/FlumeData.1651751253759\n",
            "-rw-r--r--   1 root root      83052 2022-05-05 11:47 /content/salida/FlumeData.1651751253760\n",
            "-rw-r--r--   1 root root      83052 2022-05-05 11:47 /content/salida/FlumeData.1651751253761\n",
            "-rw-r--r--   1 root root      82989 2022-05-05 11:47 /content/salida/FlumeData.1651751253762\n",
            "-rw-r--r--   1 root root      82970 2022-05-05 11:47 /content/salida/FlumeData.1651751253763\n",
            "-rw-r--r--   1 root root      82970 2022-05-05 11:47 /content/salida/FlumeData.1651751253764\n",
            "-rw-r--r--   1 root root      82960 2022-05-05 11:47 /content/salida/FlumeData.1651751253765\n",
            "-rw-r--r--   1 root root      82979 2022-05-05 11:47 /content/salida/FlumeData.1651751253766\n",
            "-rw-r--r--   1 root root      83011 2022-05-05 11:47 /content/salida/FlumeData.1651751253767\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls /content/salida | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i5QPTdZdNZS",
        "outputId": "efe88694-0033-45eb-efdb-7e6a02d721fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No,16.6,Yes,No,No,3.0,30.0,No,Female,55-59,White,Yes,Yes,Very good,5.0,Yes,No,Yes\r\n",
            "No,20.34,No,No,Yes,0.0,0.0,No,Female,80 or older,White,No,Yes,Very good,7.0,No,No,No\r\n",
            "No,26.58,Yes,No,No,20.0,30.0,No,Male,65-69,White,Yes,Yes,Fair,8.0,Yes,No,No\r\n",
            "No,24.21,No,No,No,0.0,0.0,No,Female,75-79,White,No,No,Good,6.0,No,No,Yes\r\n",
            "No,23.71,No,No,No,28.0,0.0,Yes,Female,40-44,White,No,Yes,Very good,8.0,No,No,No\r\n",
            "Yes,28.87,Yes,No,No,6.0,0.0,Yes,Female,75-79,Black,No,No,Fair,12.0,No,No,No\r\n",
            "No,21.63,No,No,No,15.0,0.0,No,Female,70-74,White,No,Yes,Fair,4.0,Yes,No,Yes\r\n",
            "No,31.64,Yes,No,No,5.0,0.0,Yes,Female,80 or older,White,Yes,No,Good,9.0,Yes,No,No\r\n",
            "No,26.45,No,No,No,0.0,0.0,No,Female,80 or older,White,\"No, borderline diabetes\",No,Fair,5.0,No,Yes,No\r\n",
            "No,40.69,No,No,No,0.0,0.0,Yes,Male,65-69,White,No,Yes,Good,10.0,No,No,No\r\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat /content/salida/FlumeData.* | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMO8TV5qYc3F"
      },
      "source": [
        "## Ejecución de PIG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gvp1o-nJFoc"
      },
      "source": [
        "### Consulta - Media y conteo del Índice de Masa Corporal (IMC) agrupado por Sexo y Edad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos el fichero de comandos de PIG.\n",
        "\n",
        "Primero obtenemos los datos de los ficheros del directorio *salida* que ha generado Flume en los pasos anteriores.<br>\n",
        "Para esta carga en PIG, le indicamos los nombres de las columnas y el tipo de cada uno de los datos, que serán del tipo *chararray* o *float*.<br>\n",
        "Continuamos aplicando un filtro *HeartDisease == 'Yes'* donde solo nos quedamos con aquellas personas que tienen detectada una anomalía coronaria.<br>\n",
        "Los datos los vamos a agrupar por Sexo y Edad *(Sex, AgeCategory)*.<br>\n",
        "Transformamos el contenido, recorriendo los datos aplicando el filtro y la agrupación y generando como salida solo 4 campos: el Sexo, campo *Sex*, la Edad, campo *AgeCategory*, y la media y el conteo del IMC, campo *BMI* (Body Mass Index - Índice de Masa Corporal en inglés).<br>\n",
        "Por último, una vez transformado el contenido, se carga en fichero separado por comas en el directorio *result*."
      ],
      "metadata": {
        "id": "qMqt-L8Db7Wx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhpaaphQupfj",
        "outputId": "a470d0c6-fb45-4cb3-fc87-c4e139693611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coronarias.pig\n"
          ]
        }
      ],
      "source": [
        "%%writefile coronarias.pig\n",
        "-- Cargar datos desde el csv\n",
        "datosEntrada = LOAD '/content/salida/FlumeData.*' USING PigStorage(',') AS \n",
        "(HeartDisease:chararray, BMI:float, Smoking:chararray, AlcoholDrinking:chararray, Stroke:chararray,\n",
        " PhysicalHealth:float, MentalHealth:float, DiffWalking:chararray, Sex:chararray, AgeCategory:chararray, Race:chararray,\n",
        " Diabetic:chararray, PhysicalActivity:chararray, GenHealth:chararray, SleepTime:float,\n",
        " Asthma:chararray, KidneyDisease:chararray, SkinCancer:chararray);\n",
        "\n",
        "-- filtro de datos\n",
        "conEnfermedad = FILTER datosEntrada BY (HeartDisease == 'Yes');\n",
        "\n",
        "-- agrupación por varias columnas\n",
        "agrupadosSexoEdad = group conEnfermedad BY (Sex, AgeCategory);\n",
        "\n",
        "-- recorrido del contenido\n",
        "mediaBMI = FOREACH agrupadosSexoEdad GENERATE FLATTEN(group) AS (Sexo, Edad), AVG(conEnfermedad.BMI), COUNT(conEnfermedad.BMI);\n",
        "\n",
        "-- almacenar (directorio no existente)\n",
        "STORE mediaBMI INTO 'results/' USING PigStorage (',');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos el fichero PIG creado.<br>\n",
        "Durante la ejecución del fichero podemos y ir viendo como se van procesando los *map-reduce* correspondientes."
      ],
      "metadata": {
        "id": "-jLzCIFLcPB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If5VrSmlwCGT",
        "outputId": "72439599-4d6c-4ab3-81ab-19a6a2f197b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-05 11:48:37,157 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n",
            "2022-05-05 11:48:37,159 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE\n",
            "2022-05-05 11:48:37,159 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType\n",
            "2022-05-05 11:48:37,275 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n",
            "2022-05-05 11:48:37,275 [main] INFO  org.apache.pig.Main - Logging error messages to: /content/pig_1651751317261.log\n",
            "2022-05-05 11:48:37,738 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
            "2022-05-05 11:48:37,828 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-05-05 11:48:37,828 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
            "2022-05-05 11:48:37,869 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-coronarias.pig-4cc26840-29ba-42f9-b861-42a51591caef\n",
            "2022-05-05 11:48:37,870 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n",
            "2022-05-05 11:48:38,642 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
            "2022-05-05 11:48:38,661 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,FILTER\n",
            "2022-05-05 11:48:38,714 [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
            "2022-05-05 11:48:38,757 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NestedLimitOptimizer, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}\n",
            "2022-05-05 11:48:38,853 [main] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
            "2022-05-05 11:48:38,936 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
            "2022-05-05 11:48:38,958 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.CombinerOptimizerUtil - Choosing to move algebraic foreach to combiner\n",
            "2022-05-05 11:48:39,012 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
            "2022-05-05 11:48:39,012 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
            "2022-05-05 11:48:39,168 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsConfig - Loaded properties from hadoop-metrics2.properties\n",
            "2022-05-05 11:48:39,370 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-05-05 11:48:39,370 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system started\n",
            "2022-05-05 11:48:39,413 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n",
            "2022-05-05 11:48:39,422 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
            "2022-05-05 11:48:39,422 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
            "2022-05-05 11:48:39,424 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
            "2022-05-05 11:48:39,426 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
            "2022-05-05 11:48:39,428 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
            "2022-05-05 11:48:39,472 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=797964\n",
            "2022-05-05 11:48:39,473 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
            "2022-05-05 11:48:39,473 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
            "2022-05-05 11:48:39,473 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - This job cannot be converted run in-process\n",
            "2022-05-05 11:48:39,519 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
            "2022-05-05 11:48:39,617 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig-0.17.0/pig-0.17.0-core-h2.jar to DistributedCache through /tmp/temp958606041/tmp-1302503660/pig-0.17.0-core-h2.jar\n",
            "2022-05-05 11:48:39,621 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig-0.17.0/lib/automaton-1.11-8.jar to DistributedCache through /tmp/temp958606041/tmp1070143269/automaton-1.11-8.jar\n",
            "2022-05-05 11:48:39,623 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig-0.17.0/lib/antlr-runtime-3.4.jar to DistributedCache through /tmp/temp958606041/tmp-1413199719/antlr-runtime-3.4.jar\n",
            "2022-05-05 11:48:39,628 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig-0.17.0/lib/joda-time-2.9.3.jar to DistributedCache through /tmp/temp958606041/tmp-511693030/joda-time-2.9.3.jar\n",
            "2022-05-05 11:48:39,649 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
            "2022-05-05 11:48:39,655 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
            "2022-05-05 11:48:39,655 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
            "2022-05-05 11:48:39,655 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
            "2022-05-05 11:48:39,855 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
            "2022-05-05 11:48:39,871 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:39,910 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-05-05 11:48:39,969 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
            "2022-05-05 11:48:39,989 [JobControl] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
            "2022-05-05 11:48:39,999 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 10\n",
            "2022-05-05 11:48:39,999 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 10\n",
            "2022-05-05 11:48:40,022 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
            "2022-05-05 11:48:40,047 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
            "2022-05-05 11:48:40,361 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local812371900_0001\n",
            "2022-05-05 11:48:40,361 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n",
            "2022-05-05 11:48:40,858 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/job_local812371900_0001_67da357b-6afc-4273-aee0-3418537b8459/pig-0.17.0-core-h2.jar <- /content/pig-0.17.0-core-h2.jar\n",
            "2022-05-05 11:48:40,886 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp958606041/tmp-1302503660/pig-0.17.0-core-h2.jar as file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_67da357b-6afc-4273-aee0-3418537b8459/pig-0.17.0-core-h2.jar\n",
            "2022-05-05 11:48:40,922 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/job_local812371900_0001_254af085-e62d-4b96-9c21-a0815bee8b6c/automaton-1.11-8.jar <- /content/automaton-1.11-8.jar\n",
            "2022-05-05 11:48:40,940 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp958606041/tmp1070143269/automaton-1.11-8.jar as file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_254af085-e62d-4b96-9c21-a0815bee8b6c/automaton-1.11-8.jar\n",
            "2022-05-05 11:48:40,940 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/job_local812371900_0001_6940b436-96ac-494e-996e-97bd7ada63e9/antlr-runtime-3.4.jar <- /content/antlr-runtime-3.4.jar\n",
            "2022-05-05 11:48:40,952 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp958606041/tmp-1413199719/antlr-runtime-3.4.jar as file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_6940b436-96ac-494e-996e-97bd7ada63e9/antlr-runtime-3.4.jar\n",
            "2022-05-05 11:48:40,953 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/job_local812371900_0001_4fad7203-4459-42d9-a083-9f2d43bf5b64/joda-time-2.9.3.jar <- /content/joda-time-2.9.3.jar\n",
            "2022-05-05 11:48:40,963 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp958606041/tmp-511693030/joda-time-2.9.3.jar as file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_4fad7203-4459-42d9-a083-9f2d43bf5b64/joda-time-2.9.3.jar\n",
            "2022-05-05 11:48:41,079 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_67da357b-6afc-4273-aee0-3418537b8459/pig-0.17.0-core-h2.jar\n",
            "2022-05-05 11:48:41,079 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_254af085-e62d-4b96-9c21-a0815bee8b6c/automaton-1.11-8.jar\n",
            "2022-05-05 11:48:41,079 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_6940b436-96ac-494e-996e-97bd7ada63e9/antlr-runtime-3.4.jar\n",
            "2022-05-05 11:48:41,079 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/job_local812371900_0001_4fad7203-4459-42d9-a083-9f2d43bf5b64/joda-time-2.9.3.jar\n",
            "2022-05-05 11:48:41,084 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
            "2022-05-05 11:48:41,085 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local812371900_0001\n",
            "2022-05-05 11:48:41,085 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases agrupadosSexoEdad,conEnfermedad,datosEntrada,mediaBMI\n",
            "2022-05-05 11:48:41,085 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: datosEntrada[2,15],datosEntrada[-1,-1],conEnfermedad[9,16],mediaBMI[15,11],agrupadosSexoEdad[12,20] C: mediaBMI[15,11],agrupadosSexoEdad[12,20] R: mediaBMI[15,11]\n",
            "2022-05-05 11:48:41,093 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
            "2022-05-05 11:48:41,108 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
            "2022-05-05 11:48:41,109 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local812371900_0001]\n",
            "2022-05-05 11:48:41,166 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
            "2022-05-05 11:48:41,168 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
            "2022-05-05 11:48:41,168 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
            "2022-05-05 11:48:41,174 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
            "2022-05-05 11:48:41,174 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-05 11:48:41,174 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
            "2022-05-05 11:48:41,307 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
            "2022-05-05 11:48:41,308 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local812371900_0001_m_000000_0\n",
            "2022-05-05 11:48:41,389 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
            "2022-05-05 11:48:41,393 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-05 11:48:41,424 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-05-05 11:48:41,434 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :10\n",
            "Total Length = 797964\n",
            "Input split[0]:\n",
            "   Length = 83052\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[1]:\n",
            "   Length = 83052\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[2]:\n",
            "   Length = 83044\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[3]:\n",
            "   Length = 83011\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[4]:\n",
            "   Length = 82989\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[5]:\n",
            "   Length = 82979\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[6]:\n",
            "   Length = 82970\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[7]:\n",
            "   Length = 82970\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[8]:\n",
            "   Length = 82960\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "Input split[9]:\n",
            "   Length = 50937\n",
            "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
            "   Locations:\n",
            "\n",
            "-----------------------\n",
            "\n",
            "2022-05-05 11:48:41,453 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
            "2022-05-05 11:48:41,459 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253760:0+83052\n",
            "2022-05-05 11:48:41,683 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-05-05 11:48:41,684 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
            "2022-05-05 11:48:41,686 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
            "2022-05-05 11:48:41,686 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
            "2022-05-05 11:48:41,686 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
            "2022-05-05 11:48:41,695 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-05-05 11:48:41,730 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
            "2022-05-05 11:48:41,732 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
            "2022-05-05 11:48:41,765 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: datosEntrada[2,15],datosEntrada[-1,-1],conEnfermedad[9,16],mediaBMI[15,11],agrupadosSexoEdad[12,20] C: mediaBMI[15,11],agrupadosSexoEdad[12,20] R: mediaBMI[15,11]\n",
            "2022-05-05 11:48:41,970 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253761:0+83052\n",
            "2022-05-05 11:48:42,060 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253759:0+83044\n",
            "2022-05-05 11:48:42,129 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253767:0+83011\n",
            "2022-05-05 11:48:42,195 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253762:0+82989\n",
            "2022-05-05 11:48:42,239 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253766:0+82979\n",
            "2022-05-05 11:48:42,280 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253763:0+82970\n",
            "2022-05-05 11:48:42,335 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253764:0+82970\n",
            "2022-05-05 11:48:42,376 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253765:0+82960\n",
            "2022-05-05 11:48:42,414 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/salida/FlumeData.1651751253768:0+50937\n",
            "2022-05-05 11:48:42,455 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
            "2022-05-05 11:48:42,455 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
            "2022-05-05 11:48:42,455 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
            "2022-05-05 11:48:42,456 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 35554; bufvoid = 104857600\n",
            "2022-05-05 11:48:42,456 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26210456(104841824); length = 3941/6553600\n",
            "2022-05-05 11:48:42,506 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine - Aliases being processed per job phase (AliasName[line,offset]): M: datosEntrada[2,15],datosEntrada[-1,-1],conEnfermedad[9,16],mediaBMI[15,11],agrupadosSexoEdad[12,20] C: mediaBMI[15,11],agrupadosSexoEdad[12,20] R: mediaBMI[15,11]\n",
            "2022-05-05 11:48:42,545 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
            "2022-05-05 11:48:42,561 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local812371900_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-05-05 11:48:42,564 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
            "2022-05-05 11:48:42,564 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local812371900_0001_m_000000_0' done.\n",
            "2022-05-05 11:48:42,574 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local812371900_0001_m_000000_0: Counters: 19\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6606195\n",
            "\t\tFILE: Number of bytes written=12394261\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=986\n",
            "\t\tMap output bytes=35554\n",
            "\t\tMap output materialized bytes=990\n",
            "\t\tInput split bytes=956\n",
            "\t\tCombine input records=986\n",
            "\t\tCombine output records=25\n",
            "\t\tSpilled Records=25\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=21\n",
            "\t\tTotal committed heap usage (bytes)=294649856\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=0\n",
            "\torg.apache.pig.PigWarning\n",
            "\t\tFIELD_DISCARDED_TYPE_CONVERSION_FAILED=241\n",
            "2022-05-05 11:48:42,574 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local812371900_0001_m_000000_0\n",
            "2022-05-05 11:48:42,574 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
            "2022-05-05 11:48:42,578 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
            "2022-05-05 11:48:42,579 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local812371900_0001_r_000000_0\n",
            "2022-05-05 11:48:42,618 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
            "2022-05-05 11:48:42,618 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-05 11:48:42,624 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-05-05 11:48:42,629 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@75e39490\n",
            "2022-05-05 11:48:42,631 [pool-4-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:42,670 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=734003200, maxSingleShuffleLimit=183500800, mergeThreshold=484442144, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-05-05 11:48:42,683 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
            "2022-05-05 11:48:42,684 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local812371900_0001]\n",
            "2022-05-05 11:48:42,686 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local812371900_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-05-05 11:48:42,741 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local812371900_0001_m_000000_0 decomp: 986 len: 990 to MEMORY\n",
            "2022-05-05 11:48:42,749 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 986 bytes from map-output for attempt_local812371900_0001_m_000000_0\n",
            "2022-05-05 11:48:42,751 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 986, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->986\n",
            "2022-05-05 11:48:42,759 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
            "2022-05-05 11:48:42,760 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
            "2022-05-05 11:48:42,761 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-05-05 11:48:42,774 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
            "2022-05-05 11:48:42,774 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 966 bytes\n",
            "2022-05-05 11:48:42,778 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 986 bytes to disk to satisfy reduce memory limit\n",
            "2022-05-05 11:48:42,779 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 990 bytes from disk\n",
            "2022-05-05 11:48:42,781 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-05-05 11:48:42,782 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
            "2022-05-05 11:48:42,783 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 966 bytes\n",
            "2022-05-05 11:48:42,784 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
            "2022-05-05 11:48:42,799 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
            "2022-05-05 11:48:42,799 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-05 11:48:42,804 [pool-4-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-05-05 11:48:42,807 [pool-4-thread-1] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
            "2022-05-05 11:48:42,808 [pool-4-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
            "2022-05-05 11:48:42,823 [pool-4-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: datosEntrada[2,15],datosEntrada[-1,-1],conEnfermedad[9,16],mediaBMI[15,11],agrupadosSexoEdad[12,20] C: mediaBMI[15,11],agrupadosSexoEdad[12,20] R: mediaBMI[15,11]\n",
            "2022-05-05 11:48:42,839 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local812371900_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-05-05 11:48:42,876 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
            "2022-05-05 11:48:42,877 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local812371900_0001_r_000000_0 is allowed to commit now\n",
            "2022-05-05 11:48:42,892 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local812371900_0001_r_000000_0' to file:/content/results\n",
            "2022-05-05 11:48:42,895 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
            "2022-05-05 11:48:42,895 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local812371900_0001_r_000000_0' done.\n",
            "2022-05-05 11:48:42,896 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local812371900_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6608207\n",
            "\t\tFILE: Number of bytes written=12396111\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=25\n",
            "\t\tReduce shuffle bytes=990\n",
            "\t\tReduce input records=25\n",
            "\t\tReduce output records=25\n",
            "\t\tSpilled Records=25\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=294649856\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=0\n",
            "2022-05-05 11:48:42,897 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local812371900_0001_r_000000_0\n",
            "2022-05-05 11:48:42,897 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
            "2022-05-05 11:48:43,185 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local812371900_0001]\n",
            "2022-05-05 11:48:46,195 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:46,221 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:46,223 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-05-05 11:48:46,228 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:46,293 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
            "2022-05-05 11:48:46,298 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: \n",
            "\n",
            "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
            "3.3.1\t0.17.0\troot\t2022-05-05 11:48:39\t2022-05-05 11:48:46\tGROUP_BY,FILTER\n",
            "\n",
            "Success!\n",
            "\n",
            "Job Stats (time in seconds):\n",
            "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTime\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
            "job_local812371900_0001\t1\t1\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tagrupadosSexoEdad,conEnfermedad,datosEntrada,mediaBMI\tGROUP_BY,COMBINER\tfile:///content/results,\n",
            "\n",
            "Input(s):\n",
            "Successfully read 10000 records from: \"/content/salida/FlumeData.*\"\n",
            "\n",
            "Output(s):\n",
            "Successfully stored 25 records in: \"file:///content/results\"\n",
            "\n",
            "Counters:\n",
            "Total records written : 25\n",
            "Total bytes written : 0\n",
            "Spillable Memory Manager spill count : 0\n",
            "Total bags proactively spilled: 0\n",
            "Total records proactively spilled: 0\n",
            "\n",
            "Job DAG:\n",
            "job_local812371900_0001\n",
            "\n",
            "\n",
            "2022-05-05 11:48:46,308 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:46,316 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:46,352 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
            "2022-05-05 11:48:46,362 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Encountered Warning FIELD_DISCARDED_TYPE_CONVERSION_FAILED 241 time(s).\n",
            "2022-05-05 11:48:46,362 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
            "2022-05-05 11:48:46,403 [main] INFO  org.apache.pig.Main - Pig script completed in 9 seconds and 322 milliseconds (9322 ms)\n"
          ]
        }
      ],
      "source": [
        "!pig coronarias.pig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos listando el contenido del fichero."
      ],
      "metadata": {
        "id": "6jNs9Q0bcWvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1boe7YXQw2QR",
        "outputId": "157c5734-7d87-40bb-9919-ec9a1b60f444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Male,18-24,30.540000915527344,1\n",
            "Male,25-29,27.963333129882812,3\n",
            "Male,30-34,36.62833213806152,6\n",
            "Male,35-39,32.24428612845285,7\n",
            "Male,40-44,28.741666793823242,6\n",
            "Male,45-49,30.614545475352894,11\n",
            "Male,50-54,32.25333302815755,30\n",
            "Male,55-59,30.192187130451202,32\n",
            "Male,60-64,30.09371449606759,70\n",
            "Male,65-69,29.114456508470617,92\n",
            "Male,70-74,29.848672562995844,113\n",
            "Male,75-79,28.38057465388857,87\n",
            "Male,80 or older,26.992307607944195,104\n",
            "Female,18-24,47.01499938964844,2\n",
            "Female,30-34,29.130000114440918,2\n",
            "Female,35-39,41.36000061035156,2\n",
            "Female,40-44,31.240000009536743,8\n",
            "Female,45-49,37.636250257492065,8\n",
            "Female,50-54,33.87199981689453,25\n",
            "Female,55-59,28.912926929753002,41\n",
            "Female,60-64,29.297777864668106,36\n",
            "Female,65-69,30.60471431187221,70\n",
            "Female,70-74,29.403466567993163,75\n",
            "Female,75-79,29.380645013624623,62\n",
            "Female,80 or older,25.646559120506367,93\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat /content/results/part*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación de los datos resultantes"
      ],
      "metadata": {
        "id": "vFdpkpsknDJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la salida podemos ver como la primera columna es el sexo, la segunda la categoría de edad en la que se encuadra, la tercera es la media del IMC y la última el numero de personas que integran ese registro.<br>\n",
        "Como explicación general de esta salida podemos indicar que todas las personas de la muestra con problemas coronarios tienen mínimo sobrepeso (IMC > 25). Si hiciesemos la media de forma global nos acercaríamos a 30, que es donde empieza la obesidad.<br>\n",
        "Por tanto, se puede determinar que un IMC elevado puede provocar problemas cardíacos. "
      ],
      "metadata": {
        "id": "v759oiupaVpR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BDA_Practica_2_Flume_PIG",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}